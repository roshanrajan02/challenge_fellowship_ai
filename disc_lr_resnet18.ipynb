{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0b7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import functools\n",
    "from typing import Union\n",
    "import argparse\n",
    "import shutil\n",
    "import random\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8116c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = '/Users/roshanrajan/Desktop/oxford-102-flowers'\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efa9db28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c540b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Developped by the Fastai team for the Fastai library\n",
    "From the fastai library\n",
    "https://www.fast.ai and https://github.com/fastai/fastai\n",
    "'''\n",
    "\n",
    "###############################################################################\n",
    "#Unmodified classes and functions:\n",
    "\n",
    "class PrePostInitMeta(type):\n",
    "    \"A metaclass that calls optional `__pre_init__` and `__post_init__` methods\"\n",
    "    def __new__(cls, name, bases, dct):\n",
    "        x = super().__new__(cls, name, bases, dct)\n",
    "        old_init = x.__init__\n",
    "        def _pass(self): pass\n",
    "        @functools.wraps(old_init)\n",
    "        def _init(self,*args,**kwargs):\n",
    "            self.__pre_init__()\n",
    "            old_init(self, *args,**kwargs)\n",
    "            self.__post_init__()\n",
    "        x.__init__ = _init\n",
    "        if not hasattr(x,'__pre_init__'):  x.__pre_init__  = _pass\n",
    "        if not hasattr(x,'__post_init__'): x.__post_init__ = _pass\n",
    "        return x\n",
    "\n",
    "class Module(nn.Module, metaclass=PrePostInitMeta):\n",
    "    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n",
    "    def __pre_init__(self): super().__init__()\n",
    "    def __init__(self): pass\n",
    "\n",
    "class ParameterModule(Module):\n",
    "    \"Register a lone parameter `p` in a module.\"\n",
    "    def __init__(self, p:nn.Parameter): self.val = p\n",
    "    def forward(self, x): return x\n",
    "\n",
    "def children(m:nn.Module):\n",
    "    \"Get children of `m`.\"\n",
    "    return list(m.children())\n",
    "\n",
    "def num_children(m:nn.Module):\n",
    "    \"Get number of children modules in `m`.\"\n",
    "    return len(children(m))\n",
    "\n",
    "def children_and_parameters(m:nn.Module):\n",
    "    \"Return the children of `m` and its direct parameters not registered in modules.\"\n",
    "    children = list(m.children())\n",
    "    children_p = sum([[id(p) for p in c.parameters()] for c in m.children()],[])\n",
    "    for p in m.parameters():\n",
    "        if id(p) not in children_p: children.append(ParameterModule(p))\n",
    "    return children\n",
    "\n",
    "def even_mults(start:float, stop:float, n:int)->np.ndarray:\n",
    "    \"Build log-stepped array from `start` to `stop` in `n` steps.\"\n",
    "    mult = stop/start\n",
    "    step = mult**(1/(n-1))\n",
    "    return np.array([start*(step**i) for i in range(n)])\n",
    "\n",
    "flatten_model = lambda m: sum(map(flatten_model,children_and_parameters(m)),[]) if num_children(m) else [m]\n",
    "###############################################################################\n",
    "\n",
    "'''\n",
    "Modified version of lr_range from fastai\n",
    "https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L185\n",
    "'''\n",
    "def lr_range(net:nn.Module, lr:slice, model_len:int)->np.ndarray:\n",
    "        \"Build differential learning rates from `lr`.\"\n",
    "\n",
    "        if not isinstance(lr,slice): return lr\n",
    "        if lr.start: res = even_mults(lr.start, lr.stop, model_len)\n",
    "        else: res = [lr.stop/10]*(model_len-1) + [lr.stop]\n",
    "        return res\n",
    "\n",
    "def unfreeze_layers(model:nn.Sequential, unfreeze:bool=True)->None:\n",
    "    \"Unfreeze or freeze all layers\"\n",
    "\n",
    "    for layer in model.parameters():\n",
    "        layer.requires_grad = unfreeze\n",
    "\n",
    "def build_param_dicts(layers:nn.Sequential, lr:list=[0], return_len:bool=False)->Union[int,list]:\n",
    "    '''\n",
    "    Either return the number of layers with requires_grad is True\n",
    "    or return a list of dictionnaries containing each layers on its associated LR\"\n",
    "    Both weight and bias are check for requires_grad is True\n",
    "    '''\n",
    "\n",
    "    params = []\n",
    "    idx = 0\n",
    "    for layer in layers:\n",
    "        param = []\n",
    "        if (hasattr(layer, \"requires_grad\") and layer.requires_grad):\n",
    "            #To implement for custom nn.Parameter()\n",
    "            print(\"Custom nn.Parameter() not supported\")\n",
    "        if(hasattr(layer, \"weight\") and layer.weight.requires_grad):\n",
    "            param.append(layer.weight)\n",
    "        if (hasattr(layer, \"bias\") and hasattr(layer.bias, \"requires_grad\") and layer.bias.requires_grad):\n",
    "            param.append(layer.bias)\n",
    "        if param: params.append({'params': param, 'lr': f'{lr[idx]}'}); idx += 1\n",
    "        if return_len: idx = 0 #We don't want to increment idx here.\n",
    "\n",
    "    return len(params) if return_len else params\n",
    "\n",
    "def discriminative_lr_params(net:nn.Module, lr:slice, unfreeze:bool=True)->Union[list,np.ndarray,nn.Sequential]:\n",
    "    '''\n",
    "    Flatten our model and generate a list of dictionnaries to be passed to the\n",
    "    optimizer.\n",
    "    - If only one learning rate is passed as a slice the last layer will have the\n",
    "    corresponding learning rate and all other ones will have lr/10\n",
    "    - If two learning rates are passed such as slice(min_lr, max_lr) the last\n",
    "    layer will have max_lr as a learning rate and the first one will have min_lr.\n",
    "    All middle layers will have learning rates logarithmically interpolated\n",
    "    ranging from min_lr to max_lr\n",
    "    '''\n",
    "\n",
    "    layers = nn.Sequential(*flatten_model(net)) #Flatten/ungroup our model\n",
    "    if unfreeze: unfreeze_layers(layers, True)  #Unfreeze all layers\n",
    "\n",
    "    #Return the number of layer where requires_grad is True (bias + weight)\n",
    "    model_len = build_param_dicts(layers, return_len=True)\n",
    "\n",
    "    #Create the list of learning rates\n",
    "    list_lr = lr_range(net, lr, model_len)\n",
    "\n",
    "    #Create our optimizer parameters list of dictionnaries\n",
    "    params_layers = build_param_dicts(layers, list_lr)\n",
    "\n",
    "    return params_layers, np.array(list_lr), layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "211afd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "params, lr_arr, _ = discriminative_lr_params(model, slice(1e-5, 1e-3))\n",
    "optim = torch.optim.SGD(params, lr=1e-3, momentum=0.9, weight_decay=1e-1)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CyclicLR(optim, base_lr=list(lr_arr), max_lr=list(lr_arr*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4a2f8d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'train_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ls/djl88lkx7db6hv9t27zztvk80000gn/T/ipykernel_7488/2929932356.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'train_model'"
     ]
    }
   ],
   "source": [
    "model.train_model(model, criterion, optimizer, lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a4398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
